{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='red'> Spam SMS Classifier<font>\n",
    "### Machine learning is so powerful that it is helping us in each possible area and saving precious time. One of the use casesÂ of Machine learning is Spam SMS detection, Using Natural Language Processing we can easily classify Spam and non Spam SMS.\n",
    "# <font color='green'> Download data for spam detection<font>\n",
    "## https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection\n",
    "# <font color='red'> Data Description : <font> \n",
    "### The collection is composed by just one text file, where each line has the correct class followed by the raw message. \n",
    "### Ham : Not spam and Spam : Spam\n",
    " A subset of 5572 SMS randomly chosen ham messages of the NUS SMS Corpus (NSC), which is a dataset of about 10,000 legitimate messages collected for research at the Department of Computer Science at the National University of Singapore. The messages largely originate from Singaporeans and mostly from students attending the University. These messages were collected from volunteers who were made aware that their contributions were going to be made publicly available.\n",
    "# <font color='red'> Objective : <font> \n",
    "### Using machine learning model, our model should be able to classify Spam and normal SMS.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms_data = pd.read_csv('smsspamcollection/SMSSpamCollection',sep='\\t', names=['label','message'])\n",
    "sms_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ham means not spam and spam is spam SMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 5572 SMS\n"
     ]
    }
   ],
   "source": [
    "# Getting Size of data\n",
    "sms_data.shape\n",
    "print(\"We have 5572 SMS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     4825\n",
       "spam     747\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if data is balanced or not\n",
    "sms_data['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'> This dataset is imbalanced , because number of sample of class ham is significantly larger than number of sample of class spam. But we have 747 spam, we will be able to classify SMS.<font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'down', 'aren', 'this', 'nor', 'mustn', 'under', 'shan', 'have', 'when', \"isn't\", \"didn't\", 'of', 'wasn', 'your', 'is', 'once', 'yourselves', 'can', 'he', 'out', 'isn', \"mightn't\", 'other', \"aren't\", 'had', 'how', 'being', 'but', \"it's\", 'was', 'above', 'mightn', 'into', 'now', 't', 'to', 'in', 'does', 'haven', \"shouldn't\", 'them', 'a', 'o', 'ourselves', 'hers', \"couldn't\", 'hasn', 'while', 'which', 'itself', 'him', 'such', 'be', 'yours', 's', 'if', 'whom', 'than', 'over', 'theirs', 'and', 'here', 'are', 'she', 'then', 'been', 'me', 'off', 'that', 'hadn', 'couldn', 'didn', 'after', 'll', 'further', 'm', 'myself', 'his', 've', 'no', 'it', 'as', 'from', 'we', 'few', 'because', 'my', 'ours', 'their', 'against', 'before', \"don't\", 'who', 'y', \"wasn't\", 'they', \"you've\", 'between', 'wouldn', \"weren't\", 'an', 'has', 'there', \"that'll\", 'below', 'not', \"won't\", 'both', \"you'd\", 'during', 'very', \"hasn't\", 'am', 'the', 'do', 'at', 'don', \"wouldn't\", 'i', 'these', 'shouldn', \"hadn't\", 'any', 'd', \"mustn't\", 're', 'each', 'having', 'themselves', \"you'll\", 'you', 'needn', \"shan't\", 'again', 'were', 'for', 'yourself', 'with', 'ain', 'did', 'up', 'until', 'only', 'its', 'some', 'so', \"she's\", \"needn't\", 'should', 'weren', \"haven't\", 'too', 'about', \"you're\", 'ma', 'her', 'what', 'those', 'will', 'on', 'just', 'most', 'by', \"doesn't\", 'doesn', 'own', 'through', 'our', 'won', 'same', 'or', 'herself', 'all', 'where', 'why', \"should've\", 'doing', 'himself', 'more'}\n",
      "There are  179  in english\n"
     ]
    }
   ],
   "source": [
    "# making set of stopwords (Those words which do not play significant role in decision making of text processing)\n",
    "stopwords = set(stopwords.words('english'))\n",
    "print(stopwords)\n",
    "print(\"There are \",len(stopwords),\" in english\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='green'> Text Preprocessing <font>\n",
    "<ul>\n",
    "  <li>Stemming : processing of removing suffix from words to bring into root form, ex: going,gone,goes changes to root word go.</li>\n",
    "  <li>Removing punctuations, take only alphabates.</li>\n",
    "  <li>Convert words into lowercase.</li>\n",
    "    <li>Remove stopwords because they don't play significant role.</li>\n",
    "    <li>Make coupus : list of documents</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intialize stemmer object\n",
    "stemmer = PorterStemmer()\n",
    "corpus = []\n",
    "for i in range(len(sms_data)):\n",
    "    # replace non-alphabates with space\n",
    "    row_data = re.sub('[^a-zA-Z]',\" \",sms_data['message'][i])\n",
    "    # convert words into lowercase\n",
    "    row_data = row_data.lower()\n",
    "    # make list from sentences\n",
    "    row_data_list = row_data.split()\n",
    "    # remove stopwords\n",
    "    important_row_data = [stemmer.stem(word) for word in row_data_list if word not in stopwords]\n",
    "    data = ''.join(important_row_data)\n",
    "    # append to corpus\n",
    "    corpus.append(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='green'> Using \"Bag of Words\"  method <font>\n",
    "<ul>\n",
    "  <li>After text preprocessing , make differnt dimensions for differnt each words in corpus</li>\n",
    "  <li>If unique words in corpus is d and there are n documents,make a matrix of nxd.</li>\n",
    "  <li>Each sentence will  be represented by a d-dimension vectors.</li>\n",
    "    <li>Matrix will be sparse matrix</li>\n",
    "</ul>\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5572, 5053)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer()\n",
    "X = cv.fit_transform(corpus).toarray()\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 5053 different words,but many of them have very less frequency , lets take only top 3000 most frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of X :  (5572, 5053)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's take only top 2500 most frequent words, it will reduce time complexit\n",
    "cv = CountVectorizer()\n",
    "X = cv.fit_transform(corpus).toarray()\n",
    "print(\"Size of X : \",X.shape)\n",
    "X[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## label have to classes , ham and spam. but machine should be given numeric value,let's convert it into numeric form, 0 ham and 1 for spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of y :  (5572, 1)\n",
      "Converted label :\n",
      "   spam\n",
      "0     0\n",
      "1     0\n",
      "2     1\n",
      "3     0\n",
      "4     0\n",
      "Original label :\n",
      "0     ham\n",
      "1     ham\n",
      "2    spam\n",
      "3     ham\n",
      "4     ham\n",
      "Name: label, dtype: object\n"
     ]
    }
   ],
   "source": [
    "y = pd.get_dummies(sms_data['label'],drop_first=True)\n",
    "print(\"size of y : \",y.shape)\n",
    "print(\"Converted label :\")\n",
    "print(y[:5])\n",
    "print('Original label :')\n",
    "print(sms_data['label'][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='green'> Splitting data into train and test set <font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.33,random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='green'> Using Naive Bayes classifier <font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Arpit\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "spam_model = MultinomialNB().fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='green'> Prediction <font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = spam_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='green'> Analyzing model <font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1597    0]\n",
      " [ 236    6]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test,y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model achieved  87.167 % accuracy.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(y_test,y_pred)\n",
    "print(\"Model achieved \",round(accuracy*100,3) ,\"% accuracy.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      1.00      0.93      1597\n",
      "           1       1.00      0.02      0.05       242\n",
      "\n",
      "    accuracy                           0.87      1839\n",
      "   macro avg       0.94      0.51      0.49      1839\n",
      "weighted avg       0.89      0.87      0.82      1839\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
